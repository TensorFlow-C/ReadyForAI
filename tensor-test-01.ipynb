{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f00c441",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-05T02:39:21.163135Z",
     "iopub.status.busy": "2022-08-05T02:39:21.162624Z",
     "iopub.status.idle": "2022-08-05T02:39:21.177603Z",
     "shell.execute_reply": "2022-08-05T02:39:21.176453Z"
    },
    "papermill": {
     "duration": 0.022096,
     "end_time": "2022-08-05T02:39:21.180106",
     "exception": false,
     "start_time": "2022-08-05T02:39:21.158010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ad527e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T02:39:21.185402Z",
     "iopub.status.busy": "2022-08-05T02:39:21.185001Z",
     "iopub.status.idle": "2022-08-05T02:45:15.177626Z",
     "shell.execute_reply": "2022-08-05T02:45:15.176509Z"
    },
    "papermill": {
     "duration": 353.997773,
     "end_time": "2022-08-05T02:45:15.179885",
     "exception": false,
     "start_time": "2022-08-05T02:39:21.182112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n",
      "(60000, 28, 28) (60000,)\n",
      "batch: (128, 28, 28) (128,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.31148362159729 0.19664371013641357\n",
      "0 100 loss: 0.5704329609870911 19.855640411376953\n",
      "0 200 loss: 0.4719923734664917 27.713157653808594\n",
      "0 300 loss: 0.37554383277893066 20.316999435424805\n",
      "0 400 loss: 0.30097681283950806 22.38500213623047\n",
      "0 test acc: 0.8419\n",
      "1 0 loss: 0.3415146470069885 23.895103454589844\n",
      "1 100 loss: 0.40937361121177673 26.20290184020996\n",
      "1 200 loss: 0.3888799846172333 19.837717056274414\n",
      "1 300 loss: 0.3544280529022217 21.888479232788086\n",
      "1 400 loss: 0.4407181143760681 24.639982223510742\n",
      "1 test acc: 0.856\n",
      "2 0 loss: 0.3343318998813629 26.261619567871094\n",
      "2 100 loss: 0.29539400339126587 30.94114112854004\n",
      "2 200 loss: 0.3833896815776825 32.51634979248047\n",
      "2 300 loss: 0.2951754629611969 31.745765686035156\n",
      "2 400 loss: 0.34952977299690247 32.3825569152832\n",
      "2 test acc: 0.8568\n",
      "3 0 loss: 0.4023056626319885 31.629701614379883\n",
      "3 100 loss: 0.29738283157348633 31.148456573486328\n",
      "3 200 loss: 0.2722136080265045 34.038726806640625\n",
      "3 300 loss: 0.32366055250167847 35.51106262207031\n",
      "3 400 loss: 0.21915803849697113 30.820425033569336\n",
      "3 test acc: 0.8773\n",
      "4 0 loss: 0.3878065347671509 35.28335952758789\n",
      "4 100 loss: 0.29436010122299194 28.504581451416016\n",
      "4 200 loss: 0.24793928861618042 38.136016845703125\n",
      "4 300 loss: 0.31777870655059814 36.066383361816406\n",
      "4 400 loss: 0.22857260704040527 34.37306213378906\n",
      "4 test acc: 0.8765\n",
      "5 0 loss: 0.40772122144699097 36.05888748168945\n",
      "5 100 loss: 0.31361865997314453 40.563995361328125\n",
      "5 200 loss: 0.28760117292404175 42.208839416503906\n",
      "5 300 loss: 0.1879538744688034 41.789405822753906\n",
      "5 400 loss: 0.23895683884620667 37.89512634277344\n",
      "5 test acc: 0.8805\n",
      "6 0 loss: 0.238959401845932 45.28791046142578\n",
      "6 100 loss: 0.22783711552619934 46.166038513183594\n",
      "6 200 loss: 0.278142511844635 44.454795837402344\n",
      "6 300 loss: 0.29437029361724854 37.12668228149414\n",
      "6 400 loss: 0.2602164149284363 48.074886322021484\n",
      "6 test acc: 0.8803\n",
      "7 0 loss: 0.24883662164211273 49.02898025512695\n",
      "7 100 loss: 0.31280308961868286 55.93409729003906\n",
      "7 200 loss: 0.22528520226478577 52.04780197143555\n",
      "7 300 loss: 0.281680703163147 49.27150344848633\n",
      "7 400 loss: 0.30766111612319946 40.297264099121094\n",
      "7 test acc: 0.8779\n",
      "8 0 loss: 0.30188971757888794 51.693092346191406\n",
      "8 100 loss: 0.30667412281036377 65.89114379882812\n",
      "8 200 loss: 0.2976049780845642 62.40467834472656\n",
      "8 300 loss: 0.35289883613586426 61.379188537597656\n",
      "8 400 loss: 0.22139278054237366 60.00571823120117\n",
      "8 test acc: 0.8884\n",
      "9 0 loss: 0.2411530762910843 62.01802444458008\n",
      "9 100 loss: 0.24252727627754211 59.951316833496094\n",
      "9 200 loss: 0.19522616267204285 72.45433044433594\n",
      "9 300 loss: 0.33426690101623535 57.19353103637695\n",
      "9 400 loss: 0.2923853397369385 62.72602462768555\n",
      "9 test acc: 0.8795\n",
      "10 0 loss: 0.1713806539773941 52.521484375\n",
      "10 100 loss: 0.15749812126159668 56.84120178222656\n",
      "10 200 loss: 0.19700074195861816 68.98493957519531\n",
      "10 300 loss: 0.1973198652267456 66.67292785644531\n",
      "10 400 loss: 0.2702403664588928 68.32481384277344\n",
      "10 test acc: 0.8836\n",
      "11 0 loss: 0.33703798055648804 71.88259887695312\n",
      "11 100 loss: 0.17823541164398193 76.04075622558594\n",
      "11 200 loss: 0.11393031477928162 79.69451141357422\n",
      "11 300 loss: 0.26648664474487305 75.06382751464844\n",
      "11 400 loss: 0.17941412329673767 83.9857177734375\n",
      "11 test acc: 0.8874\n",
      "12 0 loss: 0.1708105504512787 79.35592651367188\n",
      "12 100 loss: 0.22000068426132202 87.3822021484375\n",
      "12 200 loss: 0.20216268301010132 83.29689025878906\n",
      "12 300 loss: 0.22641003131866455 87.795654296875\n",
      "12 400 loss: 0.253192663192749 78.2245864868164\n",
      "12 test acc: 0.885\n",
      "13 0 loss: 0.14202678203582764 73.61283874511719\n",
      "13 100 loss: 0.1942339837551117 102.2891616821289\n",
      "13 200 loss: 0.4187577962875366 80.8226318359375\n",
      "13 300 loss: 0.21148571372032166 84.74835968017578\n",
      "13 400 loss: 0.19513674080371857 93.93309783935547\n",
      "13 test acc: 0.8842\n",
      "14 0 loss: 0.21637658774852753 102.14501953125\n",
      "14 100 loss: 0.23075006902217865 107.19042205810547\n",
      "14 200 loss: 0.24845045804977417 100.91629791259766\n",
      "14 300 loss: 0.17687496542930603 91.24794006347656\n",
      "14 400 loss: 0.1162174791097641 93.97178649902344\n",
      "14 test acc: 0.8915\n",
      "15 0 loss: 0.22694367170333862 93.44720458984375\n",
      "15 100 loss: 0.21857641637325287 95.87700653076172\n",
      "15 200 loss: 0.20311860740184784 109.27822875976562\n",
      "15 300 loss: 0.15970337390899658 85.85621643066406\n",
      "15 400 loss: 0.15672563016414642 95.14730834960938\n",
      "15 test acc: 0.8849\n",
      "16 0 loss: 0.1767847239971161 122.83836364746094\n",
      "16 100 loss: 0.14879210293293 114.5864486694336\n",
      "16 200 loss: 0.19923430681228638 94.48744201660156\n",
      "16 300 loss: 0.13234418630599976 119.95124053955078\n",
      "16 400 loss: 0.22740283608436584 98.99882507324219\n",
      "16 test acc: 0.8864\n",
      "17 0 loss: 0.14359118044376373 128.76040649414062\n",
      "17 100 loss: 0.18873479962348938 117.783447265625\n",
      "17 200 loss: 0.1351221352815628 111.12255859375\n",
      "17 300 loss: 0.14484858512878418 133.53945922851562\n",
      "17 400 loss: 0.2412731647491455 110.8666000366211\n",
      "17 test acc: 0.8941\n",
      "18 0 loss: 0.188820481300354 126.13896942138672\n",
      "18 100 loss: 0.15447813272476196 125.7437973022461\n",
      "18 200 loss: 0.2662437856197357 112.6961669921875\n",
      "18 300 loss: 0.15088972449302673 145.40216064453125\n",
      "18 400 loss: 0.23693016171455383 107.84828186035156\n",
      "18 test acc: 0.886\n",
      "19 0 loss: 0.20382559299468994 98.90058135986328\n",
      "19 100 loss: 0.20101574063301086 127.39326477050781\n",
      "19 200 loss: 0.20526665449142456 145.26226806640625\n",
      "19 300 loss: 0.11085455119609833 125.14103698730469\n",
      "19 400 loss: 0.16244935989379883 128.58251953125\n",
      "19 test acc: 0.8937\n",
      "20 0 loss: 0.10185331851243973 137.86962890625\n",
      "20 100 loss: 0.2338857799768448 146.19422912597656\n",
      "20 200 loss: 0.11299802362918854 123.00811767578125\n",
      "20 300 loss: 0.17248722910881042 139.8874969482422\n",
      "20 400 loss: 0.16747167706489563 131.22552490234375\n",
      "20 test acc: 0.8897\n",
      "21 0 loss: 0.13950857520103455 147.373046875\n",
      "21 100 loss: 0.14794699847698212 129.74484252929688\n",
      "21 200 loss: 0.14034876227378845 132.865478515625\n",
      "21 300 loss: 0.1451151967048645 129.074462890625\n",
      "21 400 loss: 0.2535436451435089 137.8875732421875\n",
      "21 test acc: 0.8946\n",
      "22 0 loss: 0.12165959179401398 126.78732299804688\n",
      "22 100 loss: 0.12367536127567291 168.3354949951172\n",
      "22 200 loss: 0.118873730301857 145.46871948242188\n",
      "22 300 loss: 0.14725804328918457 166.93984985351562\n",
      "22 400 loss: 0.10182514041662216 172.4100341796875\n",
      "22 test acc: 0.8906\n",
      "23 0 loss: 0.10104228556156158 165.8866424560547\n",
      "23 100 loss: 0.09204379469156265 142.47213745117188\n",
      "23 200 loss: 0.14759461581707 156.67770385742188\n",
      "23 300 loss: 0.19233384728431702 133.17572021484375\n",
      "23 400 loss: 0.1852237731218338 156.32763671875\n",
      "23 test acc: 0.8948\n",
      "24 0 loss: 0.1410869061946869 134.53944396972656\n",
      "24 100 loss: 0.12468384206295013 153.9711151123047\n",
      "24 200 loss: 0.15115468204021454 191.10153198242188\n",
      "24 300 loss: 0.13035711646080017 170.15628051757812\n",
      "24 400 loss: 0.06272466480731964 156.7164764404297\n",
      "24 test acc: 0.8893\n",
      "25 0 loss: 0.2067888379096985 183.29510498046875\n",
      "25 100 loss: 0.1763143241405487 170.69300842285156\n",
      "25 200 loss: 0.16433081030845642 208.1795654296875\n",
      "25 300 loss: 0.12814047932624817 173.42852783203125\n",
      "25 400 loss: 0.13545863330364227 199.03854370117188\n",
      "25 test acc: 0.8925\n",
      "26 0 loss: 0.13545630872249603 241.25428771972656\n",
      "26 100 loss: 0.2061396688222885 189.08384704589844\n",
      "26 200 loss: 0.09768728911876678 224.48048400878906\n",
      "26 300 loss: 0.10667141526937485 227.99928283691406\n",
      "26 400 loss: 0.2615451216697693 236.09251403808594\n",
      "26 test acc: 0.894\n",
      "27 0 loss: 0.16089603304862976 261.4596252441406\n",
      "27 100 loss: 0.21375447511672974 198.01870727539062\n",
      "27 200 loss: 0.11623847484588623 226.86427307128906\n",
      "27 300 loss: 0.12198199331760406 195.8138427734375\n",
      "27 400 loss: 0.07577335834503174 244.22433471679688\n",
      "27 test acc: 0.89\n",
      "28 0 loss: 0.1340447962284088 259.5713195800781\n",
      "28 100 loss: 0.1151045709848404 221.16226196289062\n",
      "28 200 loss: 0.2067950963973999 221.8623504638672\n",
      "28 300 loss: 0.14237651228904724 164.85374450683594\n",
      "28 400 loss: 0.13457617163658142 226.96266174316406\n",
      "28 test acc: 0.8893\n",
      "29 0 loss: 0.06709030270576477 241.9339599609375\n",
      "29 100 loss: 0.11116862297058105 265.974609375\n",
      "29 200 loss: 0.07829079777002335 254.38601684570312\n",
      "29 300 loss: 0.07958415150642395 243.1083984375\n",
      "29 400 loss: 0.1468411386013031 223.75845336914062\n",
      "29 test acc: 0.8871\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "(x, y), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "batchsz = 128\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess).batch(batchsz)\n",
    "\n",
    "db_iter = iter(db)\n",
    "sample = next(db_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu),  # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu),  # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu),  # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu),  # [b, 64] => [b, 32]\n",
    "    layers.Dense(10)  # [b, 32] => [b, 10], 330 = 32*10 + 10\n",
    "])\n",
    "model.build(input_shape=[None, 28 * 28])\n",
    "model.summary()\n",
    "# w = w - lr*grad\n",
    "optimizer = optimizers.Adam(lr=1e-3)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for epoch in range(30):\n",
    "\n",
    "        for step, (x, y) in enumerate(db):\n",
    "\n",
    "            # x: [b, 28, 28] => [b, 784]\n",
    "            # y: [b]\n",
    "            x = tf.reshape(x, [-1, 28 * 28])\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 784] => [b, 10]\n",
    "                logits = model(x)\n",
    "                y_onehot = tf.one_hot(y, depth=10)\n",
    "                # [b]\n",
    "                loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
    "                loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "                loss_ce = tf.reduce_mean(loss_ce)\n",
    "\n",
    "            grads = tape.gradient(loss_ce, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\n",
    "\n",
    "        # test\n",
    "        total_correct = 0\n",
    "        total_num = 0\n",
    "        for x, y in db_test:\n",
    "            # x: [b, 28, 28] => [b, 784]\n",
    "            # y: [b]\n",
    "            x = tf.reshape(x, [-1, 28 * 28])\n",
    "            # [b, 10]\n",
    "            logits = model(x)\n",
    "            # logits => prob, [b, 10]\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            # [b, 10] => [b], int64\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # pred:[b]\n",
    "            # y: [b]\n",
    "            # correct: [b], True: equal, False: not equal\n",
    "            correct = tf.equal(pred, y)\n",
    "            correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "\n",
    "            total_correct += int(correct)\n",
    "            total_num += x.shape[0]\n",
    "\n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, 'test acc:', acc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 366.715075,
   "end_time": "2022-08-05T02:45:18.107167",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-05T02:39:11.392092",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
