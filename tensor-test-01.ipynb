{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a888d36",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-05T02:42:43.319158Z",
     "iopub.status.busy": "2022-08-05T02:42:43.318633Z",
     "iopub.status.idle": "2022-08-05T02:42:43.332001Z",
     "shell.execute_reply": "2022-08-05T02:42:43.331104Z"
    },
    "papermill": {
     "duration": 0.020226,
     "end_time": "2022-08-05T02:42:43.334509",
     "exception": false,
     "start_time": "2022-08-05T02:42:43.314283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b477a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T02:42:43.340472Z",
     "iopub.status.busy": "2022-08-05T02:42:43.339862Z",
     "iopub.status.idle": "2022-08-05T02:53:45.858884Z",
     "shell.execute_reply": "2022-08-05T02:53:45.857364Z"
    },
    "papermill": {
     "duration": 662.525033,
     "end_time": "2022-08-05T02:53:45.861834",
     "exception": false,
     "start_time": "2022-08-05T02:42:43.336801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n",
      "(60000, 28, 28) (60000,)\n",
      "batch: (128, 28, 28) (128,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.3574166297912598 0.1684311330318451\n",
      "0 100 loss: 0.6816527843475342 15.904260635375977\n",
      "0 200 loss: 0.6259489059448242 16.044208526611328\n",
      "0 300 loss: 0.546820878982544 16.47098159790039\n",
      "0 400 loss: 0.47181785106658936 15.21959400177002\n",
      "0 test acc: 0.8351\n",
      "1 0 loss: 0.3247053623199463 17.371784210205078\n",
      "1 100 loss: 0.38357794284820557 21.1246337890625\n",
      "1 200 loss: 0.3299254775047302 22.325458526611328\n",
      "1 300 loss: 0.3408430218696594 20.916223526000977\n",
      "1 400 loss: 0.2949395477771759 24.74884033203125\n",
      "1 test acc: 0.848\n",
      "2 0 loss: 0.3936117887496948 23.173038482666016\n",
      "2 100 loss: 0.28171277046203613 28.33163833618164\n",
      "2 200 loss: 0.38846710324287415 27.031005859375\n",
      "2 300 loss: 0.33016276359558105 26.00103759765625\n",
      "2 400 loss: 0.3221728801727295 24.799327850341797\n",
      "2 test acc: 0.8679\n",
      "3 0 loss: 0.2989541292190552 26.29885482788086\n",
      "3 100 loss: 0.24050690233707428 38.100135803222656\n",
      "3 200 loss: 0.34570372104644775 35.812110900878906\n",
      "3 300 loss: 0.40438923239707947 34.30122375488281\n",
      "3 400 loss: 0.18747374415397644 36.05498504638672\n",
      "3 test acc: 0.8734\n",
      "4 0 loss: 0.3682680130004883 40.15248107910156\n",
      "4 100 loss: 0.33534711599349976 41.036163330078125\n",
      "4 200 loss: 0.3581863343715668 45.998558044433594\n",
      "4 300 loss: 0.2660217881202698 33.78520965576172\n",
      "4 400 loss: 0.2724032700061798 36.26715850830078\n",
      "4 test acc: 0.8781\n",
      "5 0 loss: 0.2747052013874054 41.319969177246094\n",
      "5 100 loss: 0.24150985479354858 44.098514556884766\n",
      "5 200 loss: 0.31153634190559387 53.27873229980469\n",
      "5 300 loss: 0.32023754715919495 54.15092468261719\n",
      "5 400 loss: 0.37556907534599304 42.27242660522461\n",
      "5 test acc: 0.8828\n",
      "6 0 loss: 0.3422117829322815 46.388916015625\n",
      "6 100 loss: 0.3321533799171448 43.59191131591797\n",
      "6 200 loss: 0.19210174679756165 50.48395919799805\n",
      "6 300 loss: 0.28949129581451416 37.620567321777344\n",
      "6 400 loss: 0.2833086848258972 57.057586669921875\n",
      "6 test acc: 0.8783\n",
      "7 0 loss: 0.26495492458343506 52.809471130371094\n",
      "7 100 loss: 0.23121076822280884 53.850563049316406\n",
      "7 200 loss: 0.21779148280620575 67.83770751953125\n",
      "7 300 loss: 0.25423094630241394 56.709014892578125\n",
      "7 400 loss: 0.2735559046268463 56.14056396484375\n",
      "7 test acc: 0.8766\n",
      "8 0 loss: 0.2530403733253479 52.661346435546875\n",
      "8 100 loss: 0.3540874123573303 52.97065734863281\n",
      "8 200 loss: 0.24632315337657928 62.47682189941406\n",
      "8 300 loss: 0.24229200184345245 63.6409912109375\n",
      "8 400 loss: 0.350347101688385 52.075904846191406\n",
      "8 test acc: 0.8772\n",
      "9 0 loss: 0.27418655157089233 57.24951171875\n",
      "9 100 loss: 0.20834429562091827 87.986328125\n",
      "9 200 loss: 0.2106582373380661 59.502376556396484\n",
      "9 300 loss: 0.3389136791229248 72.44447326660156\n",
      "9 400 loss: 0.23223386704921722 65.69845581054688\n",
      "9 test acc: 0.8838\n",
      "10 0 loss: 0.3056700527667999 81.32279205322266\n",
      "10 100 loss: 0.19334465265274048 82.61080932617188\n",
      "10 200 loss: 0.28409385681152344 74.3912353515625\n",
      "10 300 loss: 0.17119687795639038 76.41526794433594\n",
      "10 400 loss: 0.15217472612857819 77.80322265625\n",
      "10 test acc: 0.8898\n",
      "11 0 loss: 0.10452833771705627 94.01321411132812\n",
      "11 100 loss: 0.1851908415555954 87.83326721191406\n",
      "11 200 loss: 0.21711745858192444 90.377685546875\n",
      "11 300 loss: 0.2403159737586975 89.53250885009766\n",
      "11 400 loss: 0.19416218996047974 92.01272583007812\n",
      "11 test acc: 0.8866\n",
      "12 0 loss: 0.22325816750526428 76.46035766601562\n",
      "12 100 loss: 0.1290436089038849 89.71973419189453\n",
      "12 200 loss: 0.17544227838516235 75.57744598388672\n",
      "12 300 loss: 0.1550714522600174 94.80809783935547\n",
      "12 400 loss: 0.1961999088525772 77.09773254394531\n",
      "12 test acc: 0.8788\n",
      "13 0 loss: 0.2847483456134796 79.86396789550781\n",
      "13 100 loss: 0.1496615707874298 123.00807189941406\n",
      "13 200 loss: 0.1406351625919342 95.85490417480469\n",
      "13 300 loss: 0.26005664467811584 89.09303283691406\n",
      "13 400 loss: 0.20237267017364502 127.91352081298828\n",
      "13 test acc: 0.8881\n",
      "14 0 loss: 0.13072331249713898 116.43102264404297\n",
      "14 100 loss: 0.2551118731498718 122.94380950927734\n",
      "14 200 loss: 0.3213941752910614 110.54411315917969\n",
      "14 300 loss: 0.3415411114692688 82.23464965820312\n",
      "14 400 loss: 0.23285207152366638 99.83197021484375\n",
      "14 test acc: 0.8867\n",
      "15 0 loss: 0.22751061618328094 121.40232849121094\n",
      "15 100 loss: 0.14826878905296326 120.50194549560547\n",
      "15 200 loss: 0.0899478942155838 139.78994750976562\n",
      "15 300 loss: 0.15502652525901794 97.42596435546875\n",
      "15 400 loss: 0.23565620183944702 118.306884765625\n",
      "15 test acc: 0.8911\n",
      "16 0 loss: 0.17605099081993103 152.07180786132812\n",
      "16 100 loss: 0.234685480594635 107.49777221679688\n",
      "16 200 loss: 0.19304794073104858 137.520263671875\n",
      "16 300 loss: 0.26681822538375854 131.06602478027344\n",
      "16 400 loss: 0.12171337753534317 140.63629150390625\n",
      "16 test acc: 0.8793\n",
      "17 0 loss: 0.18079988658428192 141.63902282714844\n",
      "17 100 loss: 0.1570621281862259 122.77182006835938\n",
      "17 200 loss: 0.2641768455505371 151.5336151123047\n",
      "17 300 loss: 0.13808667659759521 138.98684692382812\n",
      "17 400 loss: 0.10806191712617874 159.96926879882812\n",
      "17 test acc: 0.8831\n",
      "18 0 loss: 0.2969270646572113 104.32624053955078\n",
      "18 100 loss: 0.16262999176979065 172.87234497070312\n",
      "18 200 loss: 0.17700977623462677 154.79611206054688\n",
      "18 300 loss: 0.17727258801460266 138.4065704345703\n",
      "18 400 loss: 0.15781426429748535 165.1898651123047\n",
      "18 test acc: 0.892\n",
      "19 0 loss: 0.13065452873706818 159.27195739746094\n",
      "19 100 loss: 0.18288032710552216 185.4280242919922\n",
      "19 200 loss: 0.17184676229953766 192.57174682617188\n",
      "19 300 loss: 0.19678518176078796 214.03280639648438\n",
      "19 400 loss: 0.10313951969146729 154.8207244873047\n",
      "19 test acc: 0.894\n",
      "20 0 loss: 0.184969961643219 181.1046142578125\n",
      "20 100 loss: 0.1496252417564392 244.45590209960938\n",
      "20 200 loss: 0.17576004564762115 175.25559997558594\n",
      "20 300 loss: 0.1921321153640747 148.28384399414062\n",
      "20 400 loss: 0.1504080891609192 168.79180908203125\n",
      "20 test acc: 0.8926\n",
      "21 0 loss: 0.1669524908065796 161.71670532226562\n",
      "21 100 loss: 0.14961887896060944 187.30557250976562\n",
      "21 200 loss: 0.13196828961372375 212.54656982421875\n",
      "21 300 loss: 0.2186611294746399 181.25991821289062\n",
      "21 400 loss: 0.186126708984375 138.20179748535156\n",
      "21 test acc: 0.8877\n",
      "22 0 loss: 0.134158194065094 181.85691833496094\n",
      "22 100 loss: 0.15490904450416565 214.31297302246094\n",
      "22 200 loss: 0.15555711090564728 167.84222412109375\n",
      "22 300 loss: 0.12600982189178467 197.0392303466797\n",
      "22 400 loss: 0.16858653724193573 188.37442016601562\n",
      "22 test acc: 0.8949\n",
      "23 0 loss: 0.1052788645029068 204.07064819335938\n",
      "23 100 loss: 0.1725684106349945 246.51034545898438\n",
      "23 200 loss: 0.1108502522110939 236.244384765625\n",
      "23 300 loss: 0.08493790030479431 232.9885711669922\n",
      "23 400 loss: 0.10040093958377838 198.99081420898438\n",
      "23 test acc: 0.8897\n",
      "24 0 loss: 0.12482383102178574 238.4970245361328\n",
      "24 100 loss: 0.15071973204612732 240.19308471679688\n",
      "24 200 loss: 0.1275559663772583 211.99037170410156\n",
      "24 300 loss: 0.3328667879104614 182.44580078125\n",
      "24 400 loss: 0.2052813172340393 248.170166015625\n",
      "24 test acc: 0.8907\n",
      "25 0 loss: 0.18100246787071228 187.52365112304688\n",
      "25 100 loss: 0.13433241844177246 202.89706420898438\n",
      "25 200 loss: 0.12895119190216064 291.79058837890625\n",
      "25 300 loss: 0.09570775181055069 239.53866577148438\n",
      "25 400 loss: 0.1578305959701538 218.28988647460938\n",
      "25 test acc: 0.8944\n",
      "26 0 loss: 0.08689295500516891 250.18321228027344\n",
      "26 100 loss: 0.22834502160549164 286.49652099609375\n",
      "26 200 loss: 0.05559857189655304 330.4827880859375\n",
      "26 300 loss: 0.10151730477809906 240.6500701904297\n",
      "26 400 loss: 0.15607330203056335 218.38726806640625\n",
      "26 test acc: 0.8947\n",
      "27 0 loss: 0.16879914700984955 242.42428588867188\n",
      "27 100 loss: 0.0916464775800705 289.55047607421875\n",
      "27 200 loss: 0.14872784912586212 218.48452758789062\n",
      "27 300 loss: 0.10675156116485596 373.891845703125\n",
      "27 400 loss: 0.1736464947462082 299.549560546875\n",
      "27 test acc: 0.888\n",
      "28 0 loss: 0.15911242365837097 345.8308410644531\n",
      "28 100 loss: 0.15877091884613037 269.1337890625\n",
      "28 200 loss: 0.21525433659553528 382.67852783203125\n",
      "28 300 loss: 0.13712283968925476 209.64541625976562\n",
      "28 400 loss: 0.11664703488349915 303.2166748046875\n",
      "28 test acc: 0.8929\n",
      "29 0 loss: 0.21620048582553864 310.14410400390625\n",
      "29 100 loss: 0.14190223813056946 245.93682861328125\n",
      "29 200 loss: 0.16821599006652832 213.4844970703125\n",
      "29 300 loss: 0.11972352117300034 359.0283508300781\n",
      "29 400 loss: 0.10101264715194702 300.1999816894531\n",
      "29 test acc: 0.8959\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "(x, y), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "batchsz = 128\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess).batch(batchsz)\n",
    "\n",
    "db_iter = iter(db)\n",
    "sample = next(db_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu),  # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu),  # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu),  # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu),  # [b, 64] => [b, 32]\n",
    "    layers.Dense(10)  # [b, 32] => [b, 10], 330 = 32*10 + 10\n",
    "])\n",
    "model.build(input_shape=[None, 28 * 28])\n",
    "model.summary()\n",
    "# w = w - lr*grad\n",
    "optimizer = optimizers.Adam(lr=1e-3)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for epoch in range(30):\n",
    "\n",
    "        for step, (x, y) in enumerate(db):\n",
    "\n",
    "            # x: [b, 28, 28] => [b, 784]\n",
    "            # y: [b]\n",
    "            x = tf.reshape(x, [-1, 28 * 28])\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 784] => [b, 10]\n",
    "                logits = model(x)\n",
    "                y_onehot = tf.one_hot(y, depth=10)\n",
    "                # [b]\n",
    "                loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
    "                loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "                loss_ce = tf.reduce_mean(loss_ce)\n",
    "\n",
    "            grads = tape.gradient(loss_ce, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\n",
    "\n",
    "        # test\n",
    "        total_correct = 0\n",
    "        total_num = 0\n",
    "        for x, y in db_test:\n",
    "            # x: [b, 28, 28] => [b, 784]\n",
    "            # y: [b]\n",
    "            x = tf.reshape(x, [-1, 28 * 28])\n",
    "            # [b, 10]\n",
    "            logits = model(x)\n",
    "            # logits => prob, [b, 10]\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            # [b, 10] => [b], int64\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # pred:[b]\n",
    "            # y: [b]\n",
    "            # correct: [b], True: equal, False: not equal\n",
    "            correct = tf.equal(pred, y)\n",
    "            correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "\n",
    "            total_correct += int(correct)\n",
    "            total_num += x.shape[0]\n",
    "\n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, 'test acc:', acc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 675.006178,
   "end_time": "2022-08-05T02:53:48.656353",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-05T02:42:33.650175",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
